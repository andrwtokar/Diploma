{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U5QajPl7c30"
      },
      "source": [
        "# Исследование методов классификации движений человека\n",
        "\n",
        "## Описание эксперимента\n",
        "\n",
        "В ноутбуке проведено исследование нескольких моделей на распознавание ключевых точек на теле человека.  \n",
        "Данные были взяты из датасета:\n",
        "  - LSP\n",
        "  - LSPE\n",
        "\n",
        "Для разных датасетов разная предобработка аннотаций к изображениям.\n",
        "\n",
        "Модели:  \n",
        "  - BlazePose\n",
        "  - MoveNet\n",
        "  - OpenPose\n",
        "  - MMPose\n",
        "\n",
        "Способы оценки моделей:  \n",
        "  - PCK\n",
        "  - PDJ\n",
        "  - AP с OKS в качестве метрики корректности\n",
        "\n",
        "Трешхолды для метрик PDJ и PCK: 0.05, 0.2, 0.5.  \n",
        "Трешхолды для метрикb AP: 0.5, 0.75, 0.5:0.95:0.05.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcInILXa7c35"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "!pip install mediapipe\n",
        "!pip install -q opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs\n",
        "!python -m pip install pyyaml==5.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "87kpAuT47c37"
      },
      "outputs": [],
      "source": [
        "# All imports\n",
        "import cv2\n",
        "import math\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Import matplotlib libraries\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.collections import LineCollection\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# sys utils\n",
        "import os \n",
        "import shutil\n",
        "import sys\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "# First model\n",
        "import mediapipe as mp\n",
        "\n",
        "# Second model\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_docs.vis import embed\n",
        "\n",
        "# Next models will be load from git repositories and will be import there"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnH8jyyt7c38"
      },
      "source": [
        "### Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgrwSIuiAWpO"
      },
      "outputs": [],
      "source": [
        "from google.colab import files, drive\n",
        "# files.upload()\n",
        "drive.mount('/content/drive')\n",
        "%cp -av '/content/drive/MyDrive/Диплом/LSPE/' '/content/data'\n",
        "PATH = \"/content/data/\"\n",
        "\n",
        "# можно загрузить данные на гугл диск и оттуда их подгружать в /content/\n",
        "# Директория должны выглядеть:\n",
        "# - /data\n",
        "# |- data.txt\n",
        "# |- joints.mat\n",
        "# |- /images\n",
        "#    |- 01.jpg\n",
        "#    |- 02.jpg\n",
        "#    ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XzpPl7l0JGp",
        "outputId": "0df21f91-a57b-4606-b539-aa0dcb16e6bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[  0.        ,  75.71063741, 149.31389391, ..., 154.71535102,\n",
              "          49.67604825, 118.06726708],\n",
              "        [-26.10911452,  49.4939424 , 195.50574922, ..., 133.20360364,\n",
              "          17.03758838,  23.92387506],\n",
              "        [  0.        ,   1.        ,   1.        , ...,   1.        ,\n",
              "           1.        ,   1.        ]],\n",
              "\n",
              "       [[ 26.03094352,  83.65509044, 120.93402709, ..., 160.73603675,\n",
              "          37.24532382,  97.11380604],\n",
              "        [ 86.41022512,  63.1054386 , 178.16249727, ..., 115.75988716,\n",
              "          41.3056107 ,  31.04216451],\n",
              "        [  1.        ,   1.        ,   1.        , ...,   1.        ,\n",
              "           1.        ,   1.        ]],\n",
              "\n",
              "       [[ 50.60790641,  86.64750109, 156.43016682, ..., 187.84539463,\n",
              "          52.84619531,  63.68995822],\n",
              "        [ 75.70079791,  67.02470209, 155.32224409, ..., 116.96402431,\n",
              "          33.21626993,  31.04216451],\n",
              "        [  1.        ,   1.        ,   1.        , ...,   1.        ,\n",
              "           1.        ,   1.        ]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[139.12874857, 123.72161524, 119.35736782, ..., 125.81605954,\n",
              "          63.54348958,  70.83500815],\n",
              "        [ 89.58396778, 136.1944065 , 177.39547384, ...,  57.37550775,\n",
              "         133.47412273, 108.56729432],\n",
              "        [  1.        ,   1.        ,   1.        , ...,   1.        ,\n",
              "           1.        ,   1.        ]],\n",
              "\n",
              "       [[ 96.88513933,  84.44953575, 106.74409368, ..., 171.57327105,\n",
              "          58.04648581,  27.16189395],\n",
              "        [ 56.61143933, 106.19085555, 152.16892556, ...,  68.18019781,\n",
              "          91.87179876,  69.36318138],\n",
              "        [  1.        ,   1.        ,   1.        , ...,   1.        ,\n",
              "           1.        ,   1.        ]],\n",
              "\n",
              "       [[116.56859752,  54.65783687,  87.82418246, ..., 162.55851459,\n",
              "          63.2467763 ,  16.0295315 ],\n",
              "        [ 53.42206246,  98.4847361 , 145.05265264, ...,  51.94061847,\n",
              "         101.6945697 ,  82.28849643],\n",
              "        [  1.        ,   1.        ,   1.        , ...,   1.        ,\n",
              "           1.        ,   1.        ]]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from scipy.io import loadmat\n",
        "joints = loadmat(PATH + 'joints.mat')['joints']\n",
        "joints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "h6M2Kf7q0hyy"
      },
      "outputs": [],
      "source": [
        "real_data = []\n",
        "for i in range(len(joints[0][0])):\n",
        "    photo = []\n",
        "    for j in range(12):\n",
        "        # photo.append([joints[0,j,i], joints[1,j,i], joints[2,j,i]]) # for LSP\n",
        "        photo.append([joints[j,0,i], joints[j,1,i], joints[j,2,i]]) # for LSPE\n",
        "    real_data.append(photo)\n",
        "\n",
        "real_data = np.array(real_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4o_Phps3XaQ",
        "outputId": "2654b20a-c073-4355-f692-01a8c7dc66b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[  0.        , -26.10911452,   0.        ],\n",
              "        [ 26.03094352,  86.41022512,   1.        ],\n",
              "        [ 50.60790641,  75.70079791,   1.        ],\n",
              "        ...,\n",
              "        [102.38837784,  68.46216316,   1.        ],\n",
              "        [110.78394341,  88.42703696,   1.        ],\n",
              "        [139.12874857,  89.58396778,   1.        ]],\n",
              "\n",
              "       [[ 75.71063741,  49.4939424 ,   1.        ],\n",
              "        [ 83.65509044,  63.1054386 ,   1.        ],\n",
              "        [ 86.64750109,  67.02470209,   1.        ],\n",
              "        ...,\n",
              "        [ 89.93120834,  89.63991173,   1.        ],\n",
              "        [122.74179937, 100.44436785,   1.        ],\n",
              "        [123.72161524, 136.1944065 ,   1.        ]],\n",
              "\n",
              "       [[149.31389391, 195.50574922,   1.        ],\n",
              "        [120.93402709, 178.16249727,   1.        ],\n",
              "        [156.43016682, 155.32224409,   1.        ],\n",
              "        ...,\n",
              "        [111.47407148, 145.86228848,   1.        ],\n",
              "        [120.93402709, 166.31624655,   1.        ],\n",
              "        [119.35736782, 177.39547384,   1.        ]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[154.71535102, 133.20360364,   1.        ],\n",
              "        [160.73603675, 115.75988716,   1.        ],\n",
              "        [187.84539463, 116.96402431,   1.        ],\n",
              "        ...,\n",
              "        [157.12362531,  72.99674639,   1.        ],\n",
              "        [131.83674526,  73.61508709,   1.        ],\n",
              "        [125.81605954,  57.37550775,   1.        ]],\n",
              "\n",
              "       [[ 49.67604825,  17.03758838,   1.        ],\n",
              "        [ 37.24532382,  41.3056107 ,   1.        ],\n",
              "        [ 52.84619531,  33.21626993,   1.        ],\n",
              "        ...,\n",
              "        [ 66.71363663,  89.56055854,   1.        ],\n",
              "        [ 67.58815996, 113.2507708 ,   1.        ],\n",
              "        [ 63.54348958, 133.47412273,   1.        ]],\n",
              "\n",
              "       [[118.06726708,  23.92387506,   1.        ],\n",
              "        [ 97.11380604,  31.04216451,   1.        ],\n",
              "        [ 63.68995822,  31.04216451,   1.        ],\n",
              "        ...,\n",
              "        [ 35.64496821,  74.71527871,   1.        ],\n",
              "        [ 61.04067004,  88.9786181 ,   1.        ],\n",
              "        [ 70.83500815, 108.56729432,   1.        ]]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "real_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DYp5wENofGp"
      },
      "source": [
        "### Высчитывание метрик"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aLsq02p-xOcS"
      },
      "outputs": [],
      "source": [
        "KEYPOINTS = {\n",
        "    \"right ankle\": 0,\n",
        "    \"right knee\": 1,\n",
        "    \"right hip\": 2,\n",
        "    \"left hip\": 3,\n",
        "    \"left knee\": 4,\n",
        "    \"left ankle\": 5,\n",
        "    \"right wrist\": 6,\n",
        "    \"right elbow\": 7,\n",
        "    \"right shoulder\": 8,\n",
        "    \"left shoulder\": 9,\n",
        "    \"left elbow\": 10,\n",
        "    \"left wrist\": 11,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nf8vc0QR7c3_"
      },
      "outputs": [],
      "source": [
        "# Calculate metrics\n",
        "\n",
        "# Size calculations\n",
        "\n",
        "def euclidian_metric(a, b):\n",
        "    res = np.array(a) - np.array(b)\n",
        "    return np.sqrt(np.sum(res ** 2))\n",
        "\n",
        "def calc_diag(points):\n",
        "    x_min = min(points[:, 0])\n",
        "    x_max = max(points[:, 0])\n",
        "    y_min = min(points[:, 1])\n",
        "    y_max = max(points[:, 1])\n",
        "    return euclidian_metric([x_min, y_min], [x_max, y_max])\n",
        "\n",
        "def calc_height(points):\n",
        "    y_min = min(points[:, 1])\n",
        "    y_max = max(points[:, 1])\n",
        "    return y_max - y_min\n",
        "\n",
        "def calc_square(points):\n",
        "    x_min = min(points[:, 0])\n",
        "    x_max = max(points[:, 0])\n",
        "    y_min = min(points[:, 1])\n",
        "    y_max = max(points[:, 1])\n",
        "    return (x_max - x_min) * (y_max - y_min)\n",
        "\n",
        "# Calculate PCK and PDJ\n",
        "\n",
        "def calc_pck(pred, real, threshold):\n",
        "    height = calc_height(real)\n",
        "    sum = 0\n",
        "    for p, r in zip(pred, real):\n",
        "        dist = euclidian_metric(p[:2], r[:2])\n",
        "        if dist < threshold * height:\n",
        "            sum += 1\n",
        "    return sum / len(real)\n",
        "\n",
        "def calc_pdj(pred, real, threshold):\n",
        "    diag = calc_diag(real)\n",
        "    sum = 0\n",
        "    for p, r in zip(pred, real):\n",
        "        dist = euclidian_metric(p[:2], r[:2])\n",
        "        if dist < threshold * diag:\n",
        "            sum += 1\n",
        "    return sum / len(real)\n",
        "\n",
        "# Calculate AP and mAP\n",
        "\n",
        "KEYPOINTS_OKS = {\n",
        "    \"right ankle\": 0.089,\n",
        "    \"right knee\": 0.087,\n",
        "    \"right hip\": 0.107,\n",
        "    \"left hip\": 0.107,\n",
        "    \"left knee\": 0.087,\n",
        "    \"left ankle\": 0.089,\n",
        "    \"right wrist\": 0.062,\n",
        "    \"right elbow\": 0.072,\n",
        "    \"right shoulder\": 0.079,\n",
        "    \"left shoulder\": 0.079,\n",
        "    \"left elbow\": 0.072,\n",
        "    \"left wrist\": 0.062,\n",
        "}\n",
        "\n",
        "def calc_oks(pred, real):\n",
        "    square = calc_square(real)\n",
        "    sum = 0\n",
        "    for k, p, r in zip(KEYPOINTS_OKS.values(), pred, real):\n",
        "        dist = euclidian_metric(p[:2], r[:2])\n",
        "        if r[2] > 0:\n",
        "            sum += np.exp( - dist * dist / (2 * square * k * k))\n",
        "    \n",
        "    return sum / np.sum(np.array(real)[:,2] > 0)\n",
        "\n",
        "def calc_AP(threshold, oks):\n",
        "    n = len(oks)\n",
        "    corrects = [np.array(oks) > threshold]\n",
        "    recalls = []\n",
        "    precisions = []\n",
        "    right_n = np.sum(corrects)\n",
        "    for ind in range(n):\n",
        "        precisions.append(np.sum(corrects[:i]) / i)\n",
        "        recalls.append(np.sum(corrects[:i]) / right_n)\n",
        "\n",
        "    d = {}\n",
        "    d[1] = 0\n",
        "    for r, p in zip(recalls, precisions):\n",
        "        x_min = min(d.keys())\n",
        "        if ((r in d.keys()) and (d[r] < p)) or ((not r in d.keys()) and (d[x_min] < p)):\n",
        "            d[r] = p\n",
        "    d[0] = 1\n",
        "\n",
        "    step_size = 1 / n\n",
        "    sorted_keys = sorted(d.keys())\n",
        "    sum = 0\n",
        "    for r_min, r_max in zip(sorted_keys[:-1], sorted_keys[1:]):\n",
        "        sum += round((r_max - r_min) / step_size, 1) * d[r_max]\n",
        "\n",
        "    return (sum + 1) / (n + 1)\n",
        "\n",
        "def calc_mAP(oks):\n",
        "    APs = []\n",
        "    for threshold in np.arange(0.5, 1, 0.05):\n",
        "        APs.append(calc_AP(threshold, oks))\n",
        "    \n",
        "    return np.mean(APs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AYxbExI7c39"
      },
      "source": [
        "### Первая модель: BlazePose by MediaPipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IVH7hEWQ7c38"
      },
      "outputs": [],
      "source": [
        "with open(PATH + 'data.txt') as f:\n",
        "    names = f.read().split('\\n')[:-1]\n",
        "\n",
        "images = {name: cv2.imread(PATH + 'images/' + name) for name in names}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fGess-pO7c3-"
      },
      "outputs": [],
      "source": [
        "mp_pose = mp.solutions.pose\n",
        "mp_drawing = mp.solutions.drawing_utils \n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "\n",
        "# help(mp_pose.Pose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pAAENSK7j8DK"
      },
      "outputs": [],
      "source": [
        "KEYPOINT_DICT = {\n",
        "    \"right ankle\": mp_pose.PoseLandmark.RIGHT_ANKLE,\n",
        "    \"right knee\": mp_pose.PoseLandmark.RIGHT_KNEE,\n",
        "    \"right hip\": mp_pose.PoseLandmark.RIGHT_HIP,\n",
        "    \"left hip\": mp_pose.PoseLandmark.LEFT_HIP,\n",
        "    \"left knee\": mp_pose.PoseLandmark.LEFT_KNEE,\n",
        "    \"left ankle\": mp_pose.PoseLandmark.LEFT_ANKLE,\n",
        "    \"right wrist\": mp_pose.PoseLandmark.RIGHT_WRIST,\n",
        "    \"right elbow\": mp_pose.PoseLandmark.RIGHT_ELBOW,\n",
        "    \"right shoulder\": mp_pose.PoseLandmark.RIGHT_SHOULDER,\n",
        "    \"left shoulder\": mp_pose.PoseLandmark.LEFT_SHOULDER,\n",
        "    \"left elbow\": mp_pose.PoseLandmark.LEFT_ELBOW,\n",
        "    \"left wrist\": mp_pose.PoseLandmark.LEFT_WRIST,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqoFiFuC7c3-",
        "outputId": "45f91005-d451-4e6e-e56d-1dfd00c56e3e",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Errors: 0 / 9999\n",
            "Percentage of errors: 0.0 %\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "time_landmarks = []\n",
        "errors = 0\n",
        "\n",
        "mp_real_data = []\n",
        "\n",
        "with mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5, model_complexity=1) as pose:\n",
        "    for ind, image in enumerate(images.values()):\n",
        "\n",
        "        start = time.time()\n",
        "        keypoints = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "        end = time.time()\n",
        "\n",
        "        image_height, image_width, _ = image.shape\n",
        "        if not keypoints.pose_landmarks:\n",
        "            continue\n",
        "        \n",
        "        coords = []\n",
        "        for key in KEYPOINTS.keys():\n",
        "            coords.append([\n",
        "                     keypoints.pose_landmarks.landmark[KEYPOINT_DICT[key]].x * image_width,\n",
        "                     keypoints.pose_landmarks.landmark[KEYPOINT_DICT[key]].y * image_height,\n",
        "                     keypoints.pose_landmarks.landmark[KEYPOINT_DICT[key]].visibility\n",
        "            ])\n",
        "        \n",
        "        results.append(np.array(coords))\n",
        "        time_landmarks.append(end - start)\n",
        "        mp_real_data.append(real_data[ind])\n",
        "\n",
        "print(f\"Errors: {errors} / {len(names)}\\nPercentage of errors: {errors * 100 / len(names)} %\")   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SZJVd7UQoR0Q"
      },
      "outputs": [],
      "source": [
        "# calculate metrics\n",
        "\n",
        "pck = {}\n",
        "pdj = {}\n",
        "\n",
        "for threshold in [0.05, 0.2, 0.5]:\n",
        "    pdjs = []\n",
        "    pcks = []\n",
        "    for pred, real in zip(results, mp_real_data):\n",
        "        pdjs.append(calc_pdj(pred, real, threshold))\n",
        "        pcks.append(calc_pck(pred, real, threshold))\n",
        "    \n",
        "    pck[threshold] = round(np.mean(pcks), 3)\n",
        "    pdj[threshold] = round(np.mean(pdjs), 3)\n",
        "\n",
        "oks = []\n",
        "for pred, real in zip(results, mp_real_data):\n",
        "    oks.append(calc_oks(pred, real))\n",
        "\n",
        "AP = {}\n",
        "for threshold in [0.5, 0.75]:\n",
        "    AP[threshold] = round(calc_AP(threshold, oks), 3)\n",
        "\n",
        "mAP = round(calc_mAP(oks), 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9sA6inqoR0R",
        "outputId": "143595e7-0514-44b0-fe02-a0988f44d451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pck: {0.05: 0.454, 0.2: 0.743, 0.5: 0.825}\n",
            "pdj: {0.05: 0.557, 0.2: 0.783, 0.5: 0.851}\n",
            "AP:  {0.5: 0.675, 0.75: 0.457}\n",
            "mAP: 0.443\n",
            "Average time: 0.075\n"
          ]
        }
      ],
      "source": [
        "print(f\"pck: {pck}\")\n",
        "print(f\"pdj: {pdj}\")\n",
        "print(f\"AP:  {AP}\")\n",
        "print(f\"mAP: {mAP}\")\n",
        "print(f\"Average time: {round(np.mean(time_landmarks), 3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCDHouZz7c3_"
      },
      "source": [
        "### Вторая модель: Move_Net.Singe_Pose.lightning by TensorFlowHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYYvqyBl7c3_"
      },
      "outputs": [],
      "source": [
        "model_name = \"movenet_lightning\"\n",
        "module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "\n",
        "def movenet(input_image):\n",
        "    \"\"\"Runs detection on an input image.\n",
        "\n",
        "    Args:\n",
        "        input_image: A [1, height, width, 3] tensor represents the input image\n",
        "        pixels. Note that the height/width should already be resized and match the\n",
        "        expected input resolution of the model before passing into this function.\n",
        "\n",
        "    Returns:\n",
        "        A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
        "        coordinates and scores.\n",
        "    \"\"\"\n",
        "    model = module.signatures['serving_default']\n",
        "\n",
        "    # SavedModel format expects tensor type of int32.\n",
        "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
        "    # Run model inference.\n",
        "    outputs = model(input_image)\n",
        "    # Output is a [1, 1, 17, 3] tensor.\n",
        "    keypoints_with_scores = outputs['output_0'].numpy()\n",
        "    return keypoints_with_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkyPcZ89wBON"
      },
      "outputs": [],
      "source": [
        "KEYPOINT_DICT = {\n",
        "    'nose': 0,\n",
        "    'left eye': 1,\n",
        "    'right eye': 2,\n",
        "    'left ear': 3,\n",
        "    'right ear': 4,\n",
        "    'left shoulder': 5,\n",
        "    'right shoulder': 6,\n",
        "    'left elbow': 7,\n",
        "    'right elbow': 8,\n",
        "    'left wrist': 9,\n",
        "    'right wrist': 10,\n",
        "    'left hip': 11,\n",
        "    'right hip': 12,\n",
        "    'left knee': 13,\n",
        "    'right knee': 14,\n",
        "    'left ankle': 15,\n",
        "    'right ankle': 16\n",
        "}\n",
        "\n",
        "# Maps bones to a matplotlib color name.\n",
        "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
        "    (0, 1): 'm',\n",
        "    (0, 2): 'c',\n",
        "    (1, 3): 'm',\n",
        "    (2, 4): 'c',\n",
        "    (0, 5): 'm',\n",
        "    (0, 6): 'c',\n",
        "    (5, 7): 'm',\n",
        "    (7, 9): 'm',\n",
        "    (6, 8): 'c',\n",
        "    (8, 10): 'c',\n",
        "    (5, 6): 'y',\n",
        "    (5, 11): 'm',\n",
        "    (6, 12): 'c',\n",
        "    (11, 12): 'y',\n",
        "    (11, 13): 'm',\n",
        "    (13, 15): 'm',\n",
        "    (12, 14): 'c',\n",
        "    (14, 16): 'c'\n",
        "}\n",
        "\n",
        "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
        "                                     height,\n",
        "                                     width,\n",
        "                                     keypoint_threshold=0.11):\n",
        "  \"\"\"Returns high confidence keypoints and edges for visualization.\n",
        "\n",
        "  Args:\n",
        "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
        "      the keypoint coordinates and scores returned from the MoveNet model.\n",
        "    height: height of the image in pixels.\n",
        "    width: width of the image in pixels.\n",
        "    keypoint_threshold: minimum confidence score for a keypoint to be\n",
        "      visualized.\n",
        "\n",
        "  Returns:\n",
        "    A (keypoints_xy, edges_xy, edge_colors) containing:\n",
        "      * the coordinates of all keypoints of all detected entities;\n",
        "      * the coordinates of all skeleton edges of all detected entities;\n",
        "      * the colors in which the edges should be plotted.\n",
        "  \"\"\"\n",
        "  keypoints_all = []\n",
        "  keypoint_edges_all = []\n",
        "  edge_colors = []\n",
        "  num_instances, _, _, _ = keypoints_with_scores.shape\n",
        "  for idx in range(num_instances):\n",
        "    kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
        "    kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
        "    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
        "    kpts_absolute_xy = np.stack(\n",
        "        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
        "    kpts_above_thresh_absolute = kpts_absolute_xy[\n",
        "        kpts_scores > keypoint_threshold, :]\n",
        "    keypoints_all.append(kpts_above_thresh_absolute)\n",
        "\n",
        "    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
        "      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
        "          kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
        "        x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
        "        y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
        "        x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
        "        y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
        "        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
        "        keypoint_edges_all.append(line_seg)\n",
        "        edge_colors.append(color)\n",
        "  if keypoints_all:\n",
        "    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
        "  else:\n",
        "    keypoints_xy = np.zeros((0, 17, 2))\n",
        "\n",
        "  if keypoint_edges_all:\n",
        "    edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
        "  else:\n",
        "    edges_xy = np.zeros((0, 2, 2))\n",
        "  return keypoints_xy, edges_xy, edge_colors\n",
        "\n",
        "\n",
        "def draw_prediction_on_image(\n",
        "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
        "    output_image_height=None):\n",
        "  \"\"\"Draws the keypoint predictions on image.\n",
        "\n",
        "  Args:\n",
        "    image: A numpy array with shape [height, width, channel] representing the\n",
        "      pixel values of the input image.\n",
        "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
        "      the keypoint coordinates and scores returned from the MoveNet model.\n",
        "    crop_region: A dictionary that defines the coordinates of the bounding box\n",
        "      of the crop region in normalized coordinates (see the init_crop_region\n",
        "      function below for more detail). If provided, this function will also\n",
        "      draw the bounding box on the image.\n",
        "    output_image_height: An integer indicating the height of the output image.\n",
        "      Note that the image aspect ratio will be the same as the input image.\n",
        "\n",
        "  Returns:\n",
        "    A numpy array with shape [out_height, out_width, channel] representing the\n",
        "    image overlaid with keypoint predictions.\n",
        "  \"\"\"\n",
        "  height, width, channel = image.shape\n",
        "  aspect_ratio = float(width) / height\n",
        "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
        "  # To remove the huge white borders\n",
        "  fig.tight_layout(pad=0)\n",
        "  ax.margins(0)\n",
        "  ax.set_yticklabels([])\n",
        "  ax.set_xticklabels([])\n",
        "  plt.axis('off')\n",
        "\n",
        "  im = ax.imshow(image)\n",
        "  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
        "  ax.add_collection(line_segments)\n",
        "  # Turn off tick labels\n",
        "  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
        "\n",
        "  (keypoint_locs, keypoint_edges,\n",
        "   edge_colors) = _keypoints_and_edges_for_display(\n",
        "       keypoints_with_scores, height, width)\n",
        "\n",
        "  line_segments.set_segments(keypoint_edges)\n",
        "  line_segments.set_color(edge_colors)\n",
        "  if keypoint_edges.shape[0]:\n",
        "    line_segments.set_segments(keypoint_edges)\n",
        "    line_segments.set_color(edge_colors)\n",
        "  if keypoint_locs.shape[0]:\n",
        "    scat.set_offsets(keypoint_locs)\n",
        "\n",
        "  if crop_region is not None:\n",
        "    xmin = max(crop_region['x_min'] * width, 0.0)\n",
        "    ymin = max(crop_region['y_min'] * height, 0.0)\n",
        "    rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
        "    rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
        "    rect = patches.Rectangle(\n",
        "        (xmin,ymin),rec_width,rec_height,\n",
        "        linewidth=1,edgecolor='b',facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "  fig.canvas.draw()\n",
        "  image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "  image_from_plot = image_from_plot.reshape(\n",
        "      fig.canvas.get_width_height()[::-1] + (3,))\n",
        "  plt.close(fig)\n",
        "  if output_image_height is not None:\n",
        "    output_image_width = int(output_image_height / height * width)\n",
        "    image_from_plot = cv2.resize(\n",
        "        image_from_plot, dsize=(output_image_width, output_image_height),\n",
        "         interpolation=cv2.INTER_CUBIC)\n",
        "  return image_from_plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8YG5ElOIV9H"
      },
      "outputs": [],
      "source": [
        "# read data\n",
        "with open(PATH + 'data.txt') as f:\n",
        "    names = f.read().split('\\n')[:-1]\n",
        "# images = {name: tf.image.decode_jpeg(tf.io.read_file(PATH + \"images/\" + name)) for name in names}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJItJMjIuHn1"
      },
      "outputs": [],
      "source": [
        "input_size = 192\n",
        "results = []\n",
        "time_landmarks = []\n",
        "\n",
        "for name in tqdm(names):\n",
        "    image = tf.image.decode_jpeg(tf.io.read_file(PATH + \"images/\" + name))\n",
        "\n",
        "    # Resize and pad the image to keep the aspect ratio and fit the expected size.\n",
        "    input_image = tf.expand_dims(image, axis=0)\n",
        "    input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
        "    image_height, image_width, _ = image.shape\n",
        "\n",
        "    # Run model inference.\n",
        "    start = time.time()\n",
        "    keypoints_with_scores = movenet(input_image)\n",
        "    end = time.time()\n",
        "\n",
        "    coords = []\n",
        "    for key in KEYPOINTS.keys():\n",
        "        coord = keypoints_with_scores[0,0,KEYPOINT_DICT[key]] * [image_height, image_width, 1]\n",
        "        coords.append([coord[1], coord[0], coord[2]])\n",
        "\n",
        "    results.append(np.array(coords))\n",
        "    time_landmarks.append(end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zo5e_fEP9Lde"
      },
      "outputs": [],
      "source": [
        "# calculate metrics\n",
        "\n",
        "pck = {}\n",
        "pdj = {}\n",
        "\n",
        "for threshold in [0.05, 0.2, 0.5]:\n",
        "    pdjs = []\n",
        "    pcks = []\n",
        "    for pred, real in zip(results, real_data):\n",
        "        pdjs.append(calc_pdj(pred, real, threshold))\n",
        "        pcks.append(calc_pck(pred, real, threshold))\n",
        "    \n",
        "    pck[threshold] = round(np.mean(pcks), 3)\n",
        "    pdj[threshold] = round(np.mean(pdjs), 3)\n",
        "\n",
        "oks = []\n",
        "for pred, real in zip(results, real_data):\n",
        "    oks.append(calc_oks(pred, real))\n",
        "\n",
        "AP = {}\n",
        "for threshold in [0.5, 0.75]:\n",
        "    AP[threshold] = round(calc_AP(threshold, oks), 3)\n",
        "\n",
        "mAP = round(calc_mAP(oks), 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTKzTuGo9Ldf",
        "outputId": "81a08c8f-3c19-4a2c-8a68-72a2b76f98a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pck: {0.05: 0.144, 0.2: 0.539, 0.5: 0.745}\n",
            "pdj: {0.05: 0.227, 0.2: 0.635, 0.5: 0.806}\n",
            "AP:  {0.5: 0.312, 0.75: 0.059}\n",
            "mAP: 0.111\n",
            "Average time: 0.009\n"
          ]
        }
      ],
      "source": [
        "print(f\"pck: {pck}\")\n",
        "print(f\"pdj: {pdj}\")\n",
        "print(f\"AP:  {AP}\")\n",
        "print(f\"mAP: {mAP}\")\n",
        "print(f\"Average time: {round(np.mean(time_landmarks), 3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4sVS__qw45U"
      },
      "source": [
        "### Третья модель: OpenPose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZQbISyZ7c4A"
      },
      "outputs": [],
      "source": [
        "from os.path import exists, join, basename, splitext\n",
        "\n",
        "git_repo_url = 'https://github.com/CMU-Perceptual-Computing-Lab/openpose.git'\n",
        "project_name = splitext(basename(git_repo_url))[0]\n",
        "if not exists(project_name):\n",
        "  # see: https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/949\n",
        "  # install new CMake becaue of CUDA10\n",
        "  !wget -q https://cmake.org/files/v3.13/cmake-3.13.0-Linux-x86_64.tar.gz\n",
        "  !tar xfz cmake-3.13.0-Linux-x86_64.tar.gz --strip-components=1 -C /usr/local\n",
        "  # clone openpose\n",
        "  !git clone -q --depth 1 $git_repo_url\n",
        "  !sed -i 's/execute_process(COMMAND git checkout master WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\\/3rdparty\\/caffe)/execute_process(COMMAND git checkout f019d0dfe86f49d1140961f8c7dec22130c83154 WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\\/3rdparty\\/caffe)/g' openpose/CMakeLists.txt\n",
        "  # install system dependencies\n",
        "  !apt-get -qq install -y libatlas-base-dev \n",
        "  !apt-get -qq install -y libprotobuf-dev \n",
        "  !apt-get -qq install -y libleveldb-dev \n",
        "  !apt-get -qq install -y libsnappy-dev\n",
        "  !apt-get -qq install -y libhdf5-serial-dev\n",
        "  !apt-get -qq install -y protobuf-compiler\n",
        "  !apt-get -qq install -y libgflags-dev\n",
        "  !apt-get -qq install -y libgoogle-glog-dev\n",
        "  # build openpose\n",
        "  !cd openpose && rm -rf build || true && mkdir build && cd build && cmake -DBUILD_PYTHON=ON .. && make -j`nproc`\n",
        "if not exists(\"openpose/images\"):\n",
        "    !mkdir openpose/images\n",
        "%cd openpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_O5Q7MD7c4A"
      },
      "outputs": [],
      "source": [
        "# Define directories path\n",
        "OpenposeDir = '/content/openpose/'\n",
        "\n",
        "images_list = []\n",
        "for name in names:\n",
        "    src = os.path.join(PATH, 'images', name)\n",
        "    destination = os.path.join(OpenposeDir,'images',name)\n",
        "    shutil.copy(src, destination)\n",
        "    images_list.append(destination)\n",
        "\n",
        "images_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qB28OxOsEeL"
      },
      "outputs": [],
      "source": [
        "KEYPOINT_DICT = {\n",
        "    \"right ankle\": 11,\n",
        "    \"right knee\": 10,\n",
        "    \"right hip\": 9,\n",
        "    \"left hip\": 12,\n",
        "    \"left knee\": 13,\n",
        "    \"left ankle\": 14,\n",
        "    \"right wrist\": 4,\n",
        "    \"right elbow\": 3,\n",
        "    \"right shoulder\": 2,\n",
        "    \"left shoulder\": 5,\n",
        "    \"left elbow\": 6,\n",
        "    \"left wrist\": 7,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLkrIGhR7c4A"
      },
      "outputs": [],
      "source": [
        "# Import general libraries\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# Set Python Openpose Directory for python api (Important)\n",
        "pyopenpose_dir = os.path.join(OpenposeDir,'build','python') # ex: '/content/openpose/build/python'\n",
        "if pyopenpose_dir not in sys.path:\n",
        "    sys.path.append(pyopenpose_dir)\n",
        "from openpose import pyopenpose as op\n",
        "\n",
        "# Custom Params (refer to openpose/include/openpose/flags.hpp for more parameters)\n",
        "params = dict()\n",
        "params[\"model_folder\"] = os.path.join(OpenposeDir,'models')  # ex: '/content/openpose/models'\n",
        "\n",
        "# Starting OpenPose\n",
        "opWrapper = op.WrapperPython()\n",
        "opWrapper.configure(params)\n",
        "opWrapper.start()\n",
        "\n",
        "results = []\n",
        "time_landmarks = []\n",
        "# Process Image\n",
        "for i, image in enumerate(images_list):\n",
        "    input_image = cv2.imread(image)\n",
        "    start = time.time()\n",
        "    datum = op.Datum()\n",
        "    datum.cvInputData = input_image\n",
        "    opWrapper.emplaceAndPop(op.VectorDatum([datum]))\n",
        "    network_output = datum.poseKeypoints\n",
        "    end = time.time()\n",
        "\n",
        "    coords = []\n",
        "    if not network_output is None:\n",
        "        for key in KEYPOINTS.keys():\n",
        "            sec_index = KEYPOINT_DICT[key]\n",
        "            coords.append(network_output[0, sec_index])\n",
        "    else:\n",
        "        coords = np.zeros((14,3), dtype=np.float32)\n",
        "\n",
        "    results.append(np.array(coords))\n",
        "    time_landmarks.append(end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lGWvj7NQVgu"
      },
      "outputs": [],
      "source": [
        "# calculate metrics\n",
        "\n",
        "pck = {}\n",
        "pdj = {}\n",
        "\n",
        "for threshold in [0.05, 0.2, 0.5]:\n",
        "    pdjs = []\n",
        "    pcks = []\n",
        "    for pred, real in zip(results, real_data):\n",
        "        pdjs.append(calc_pdj(pred, real, threshold))\n",
        "        pcks.append(calc_pck(pred, real, threshold))\n",
        "    \n",
        "    pck[threshold] = round(np.mean(pcks), 3)\n",
        "    pdj[threshold] = round(np.mean(pdjs), 3)\n",
        "\n",
        "oks = []\n",
        "for pred, real in zip(results, real_data):\n",
        "    oks.append(calc_oks(pred, real))\n",
        "\n",
        "AP = {}\n",
        "for threshold in [0.5, 0.75]:\n",
        "    AP[threshold] = round(calc_AP(threshold, oks), 3)\n",
        "\n",
        "mAP = round(calc_mAP(oks), 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-P3NY-KQVg7",
        "outputId": "759a44e3-9f4c-4314-dd0e-784b70e17726"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pck: {0.05: 0.362, 0.2: 0.524, 0.5: 0.633}\n",
            "pdj: {0.05: 0.42, 0.2: 0.569, 0.5: 0.69}\n",
            "AP:  {0.5: 0.538, 0.75: 0.362}\n",
            "mAP: 0.362\n",
            "Average time: 0.053\n"
          ]
        }
      ],
      "source": [
        "print(f\"pck: {pck}\")\n",
        "print(f\"pdj: {pdj}\")\n",
        "print(f\"AP:  {AP}\")\n",
        "print(f\"mAP: {mAP}\")\n",
        "print(f\"Average time: {round(np.mean(time_landmarks), 3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrEE8b5yP8oG"
      },
      "source": [
        "### Четвертая модель: MMPose by Open-MMLab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dvKWH89zw0x",
        "outputId": "2c5d1277-ffee-4c05-c744-268ded08c92c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n",
            "gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
            "Copyright (C) 2017 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n",
            "/usr/local/bin/python\n"
          ]
        }
      ],
      "source": [
        "# check NVCC version\n",
        "!nvcc -V\n",
        "\n",
        "# check GCC version\n",
        "!gcc --version\n",
        "\n",
        "# check python in conda environment\n",
        "!which python\n",
        "\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVMUH4oy7c4A"
      },
      "outputs": [],
      "source": [
        "# install dependencies: (use cu111 because colab has CUDA 11.1)\n",
        "%pip install torch==1.10.0+cu111 torchvision==0.11.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# install mmcv-full thus we could use CUDA operators\n",
        "%pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.10.0/index.html\n",
        "\n",
        "# install mmdet for inference demo\n",
        "%pip install mmdet\n",
        "\n",
        "# clone mmpose repo\n",
        "%rm -rf mmpose\n",
        "!git clone https://github.com/open-mmlab/mmpose.git\n",
        "%cd mmpose\n",
        "\n",
        "# install mmpose dependencies\n",
        "%pip install -r requirements.txt\n",
        "\n",
        "# install mmpose in develop mode\n",
        "%pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqteUccHgU1N",
        "outputId": "790852de-1886-4131-d78f-44f9d5d71a76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch version: 1.10.0+cu111 True\n",
            "torchvision version: 0.11.0+cu111\n",
            "mmpose version: 0.27.0\n",
            "cuda version: 11.1\n",
            "compiler information: GCC 7.3\n"
          ]
        }
      ],
      "source": [
        "# Check Pytorch installation\n",
        "import torch, torchvision\n",
        "\n",
        "print('torch version:', torch.__version__, torch.cuda.is_available())\n",
        "print('torchvision version:', torchvision.__version__)\n",
        "\n",
        "# Check MMPose installation\n",
        "import mmpose\n",
        "\n",
        "print('mmpose version:', mmpose.__version__)\n",
        "\n",
        "# Check mmcv installation\n",
        "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
        "\n",
        "print('cuda version:', get_compiling_cuda_version())\n",
        "print('compiler information:', get_compiler_version())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "AaUNCi28zw0z",
        "outputId": "26474279-017b-48f1-b8dd-7cec31d52df8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth\" to /root/.cache/torch/hub/checkpoints/hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "687a6e4a898a45f0a633b0372db7b0b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/243M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load checkpoint from http path: https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\" to /root/.cache/torch/hub/checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e82ea2ec0838465a929114210723a9f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/160M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from mmpose.apis import (inference_top_down_pose_model, init_pose_model,\n",
        "                         vis_pose_result, process_mmdet_results)\n",
        "from mmdet.apis import inference_detector, init_detector\n",
        "\n",
        "pose_config = 'configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/hrnet_w48_coco_256x192.py'\n",
        "pose_checkpoint = 'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth'\n",
        "det_config = 'demo/mmdetection_cfg/faster_rcnn_r50_fpn_coco.py'\n",
        "det_checkpoint = 'https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\n",
        "\n",
        "# initialize pose model\n",
        "pose_model = init_pose_model(pose_config, pose_checkpoint)\n",
        "# initialize detector\n",
        "det_model = init_detector(det_config, det_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0bfJ1AO8Kb2"
      },
      "outputs": [],
      "source": [
        "KEYPOINT_DICT = {\n",
        "    \"right ankle\": 16,\n",
        "    \"right knee\": 14,\n",
        "    \"right hip\": 12,\n",
        "    \"left hip\": 11,\n",
        "    \"left knee\": 13,\n",
        "    \"left ankle\": 15,\n",
        "    \"right wrist\": 10,\n",
        "    \"right elbow\": 8,\n",
        "    \"right shoulder\": 6,\n",
        "    \"left shoulder\": 5,\n",
        "    \"left elbow\": 7,\n",
        "    \"left wrist\": 9,\n",
        "}\n",
        "\n",
        "# \"keypoints\": [\n",
        "#             \"nose\",\"left_eye\",\"right_eye\",\"left_ear\",\"right_ear\",\n",
        "#             \"left_shoulder\",\"right_shoulder\",\"left_elbow\",\"right_elbow\",\n",
        "#             \"left_wrist\",\"right_wrist\",\"left_hip\",\"right_hip\",\n",
        "#             \"left_knee\",\"right_knee\",\"left_ankle\",\"right_ankle\"\n",
        "#         ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gcsk6DcRwBR4"
      },
      "outputs": [],
      "source": [
        "with open(PATH + 'data.txt') as f:\n",
        "    names = f.read().split('\\n')[:-1]\n",
        "\n",
        "results = []\n",
        "time_landmarks = []\n",
        "\n",
        "for name in names:\n",
        "    name = PATH + 'images/' + name\n",
        "    \n",
        "    start = time.time()\n",
        "    mmdet_results = inference_detector(det_model, name)\n",
        "    person_results = process_mmdet_results(mmdet_results, cat_id=1)\n",
        "    pose_results, returned_outputs = inference_top_down_pose_model(\n",
        "        pose_model,\n",
        "        name,\n",
        "        person_results,\n",
        "        bbox_thr=0.3,\n",
        "        format='xyxy',\n",
        "        dataset=pose_model.cfg.data.test.type)\n",
        "    end = time.time()\n",
        "    \n",
        "    if pose_results:\n",
        "        pose_results = pose_results[0]['keypoints']\n",
        "    else:\n",
        "        pose_results = np.zeros((17,3))\n",
        "\n",
        "    coords = []\n",
        "    for key in KEYPOINTS.keys():\n",
        "        coords.append(pose_results[KEYPOINT_DICT[key]])\n",
        "\n",
        "    results.append(np.array(coords))\n",
        "    time_landmarks.append(end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD3tPEe0RCME"
      },
      "outputs": [],
      "source": [
        "# calculate metrics\n",
        "\n",
        "pck = {}\n",
        "pdj = {}\n",
        "\n",
        "for threshold in [0.05, 0.2, 0.5]:\n",
        "    pdjs = []\n",
        "    pcks = []\n",
        "    for pred, real in zip(results, real_data):\n",
        "        pdjs.append(calc_pdj(pred, real, threshold))\n",
        "        pcks.append(calc_pck(pred, real, threshold))\n",
        "    \n",
        "    pck[threshold] = round(np.mean(pcks), 3)\n",
        "    pdj[threshold] = round(np.mean(pdjs), 3)\n",
        "\n",
        "oks = []\n",
        "for pred, real in zip(results, real_data):\n",
        "    oks.append(calc_oks(pred, real))\n",
        "\n",
        "AP = {}\n",
        "for threshold in [0.5, 0.75]:\n",
        "    AP[threshold] = round(calc_AP(threshold, oks), 3)\n",
        "\n",
        "mAP = round(calc_mAP(oks), 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOut2L9zRCMF",
        "outputId": "64ba4aa8-51cc-42dd-cf02-cb0c94c318d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pck: {0.05: 0.403, 0.2: 0.56, 0.5: 0.651}\n",
            "pdj: {0.05: 0.463, 0.2: 0.587, 0.5: 0.72}\n",
            "AP:  {0.5: 0.598, 0.75: 0.462}\n",
            "mAP: 0.443\n",
            "Average time: 0.436\n"
          ]
        }
      ],
      "source": [
        "print(f\"pck: {pck}\")\n",
        "print(f\"pdj: {pdj}\")\n",
        "print(f\"AP:  {AP}\")\n",
        "print(f\"mAP: {mAP}\")\n",
        "print(f\"Average time: {round(np.mean(time_landmarks), 3)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKeJkaSSQGCA"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "NnH8jyyt7c38",
        "8DYp5wENofGp",
        "7AYxbExI7c39",
        "aCDHouZz7c3_",
        "H4sVS__qw45U",
        "nrEE8b5yP8oG"
      ],
      "name": "Research_LSP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}