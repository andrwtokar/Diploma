{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U5QajPl7c30"
      },
      "source": [
        "# Исследование методов классификации движений человека\n",
        "\n",
        "## Описание эксперимента\n",
        "\n",
        "В ноутбуке проведено исследование нескольких моделей на распознавание ключевых точек на теле человека.  \n",
        "Данные были взяты из датасета Halpe.\n",
        "\n",
        "Модели:  \n",
        "  - BlazePose\n",
        "  - MoveNet\n",
        "  - OpenPose\n",
        "  - MMPose\n",
        "\n",
        "Способы оценки моделей:  \n",
        "  - PCK\n",
        "  - PDJ\n",
        "  - AP с OKS в качестве метрики корректности\n",
        "\n",
        "Трешхолды для метрик PDJ и PCK: 0.05, 0.2, 0.5.  \n",
        "Трешхолды для метрикb AP: 0.5, 0.75, 0.5:0.95:0.05.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcInILXa7c35"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "!pip install mediapipe\n",
        "!pip install -q opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs\n",
        "!python -m pip install pyyaml==5.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "87kpAuT47c37"
      },
      "outputs": [],
      "source": [
        "# All imports\n",
        "import cv2\n",
        "import math\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Import matplotlib libraries\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.collections import LineCollection\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# sys utils\n",
        "import os \n",
        "import shutil\n",
        "import sys\n",
        "import warnings\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# First model\n",
        "import mediapipe as mp\n",
        "\n",
        "# Second model\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_docs.vis import embed\n",
        "\n",
        "# Next models will be load from git repositories and will be import there"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnH8jyyt7c38"
      },
      "source": [
        "### Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgrwSIuiAWpO",
        "outputId": "df9256c2-fb6c-4a5d-98ca-a726a33dd634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "'/content/drive/MyDrive/Диплом/Halpe/halpe_train.json' -> '/content/data/halpe_train.json'\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files, drive\n",
        "# files.upload()\n",
        "drive.mount('/content/drive')\n",
        "%mkdir /content/data\n",
        "%cp -av '/content/drive/MyDrive/Диплом/Halpe/halpe_train.json' '/content/data/halpe_train.json'\n",
        "PATH = \"/content/drive/MyDrive/Диплом/Halpe/\"\n",
        "\n",
        "# можно загрузить данные на гугл диск и оттуда их подгружать в /content/\n",
        "# Директория должны выглядеть:\n",
        "# - /data\n",
        "# |- data.txt\n",
        "# |- joints.mat\n",
        "# |- /images\n",
        "#    |- 01.jpg\n",
        "#    |- 02.jpg\n",
        "#    ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XzpPl7l0JGp",
        "outputId": "f8409447-d2a9-43cf-dd9d-418314b73de7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['images', 'annotations', 'categories'])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import json\n",
        "with open('./data/halpe_train.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "M_0_q7g1o9PS"
      },
      "outputs": [],
      "source": [
        "SIZE = 30_000\n",
        "\n",
        "names = []\n",
        "real_data = []\n",
        "bboxes = []\n",
        "\n",
        "for image, annotation in zip(data['images'], data['annotations']):\n",
        "    bboxes.append(annotation['bbox'])\n",
        "\n",
        "    keypoints = []\n",
        "    for i in range(0, 408, 3):\n",
        "        keypoints.append(annotation['keypoints'][i:i+3])\n",
        "    real_data.append(keypoints[:17])\n",
        "    names.append(image['file_name'])\n",
        "\n",
        "num_repeated = {}\n",
        "for name in names:\n",
        "    if name in num_repeated.keys():\n",
        "        num_repeated[name] += 1\n",
        "    else:\n",
        "        num_repeated[name] = 1\n",
        "\n",
        "\n",
        "repeated = [k for k,v in num_repeated.items() if v>1]\n",
        "\n",
        "new_bboxes = []\n",
        "new_names = []\n",
        "new_data = []\n",
        "\n",
        "for name, bbox, keypoints in zip(names, bboxes, real_data):\n",
        "    if name not in repeated:\n",
        "        new_names.append(name)\n",
        "        new_bboxes.append(bbox)\n",
        "        new_data.append(keypoints)\n",
        "\n",
        "real_data = np.array(new_data[:SIZE])\n",
        "bboxes = np.array(new_bboxes[:SIZE])\n",
        "names = new_names[:SIZE]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eha94Gudv8dw",
        "outputId": "288bbe43-2727-4ee2-db2e-85ed2144917a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "len(np.unique(names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkodGOJ60ts-",
        "outputId": "3c97ff98-a11a-4def-d4f3-8e00121dc1e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "len(names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4o_Phps3XaQ",
        "outputId": "008f051b-5f72-48ce-bdb2-f576dffacaa4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[343, 145,   2],\n",
              "        [  0,   0,   0],\n",
              "        [219,  63,   2],\n",
              "        ...,\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0]],\n",
              "\n",
              "       [[367, 236,   2],\n",
              "        [379, 206,   2],\n",
              "        [339, 189,   2],\n",
              "        ...,\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0]],\n",
              "\n",
              "       [[215, 102,   2],\n",
              "        [212,  88,   2],\n",
              "        [192,  88,   2],\n",
              "        ...,\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        ...,\n",
              "        [484, 194,   2],\n",
              "        [473, 218,   2],\n",
              "        [482, 221,   2]],\n",
              "\n",
              "       [[334, 150,   2],\n",
              "        [336, 130,   2],\n",
              "        [  0,   0,   0],\n",
              "        ...,\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0]],\n",
              "\n",
              "       [[  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        ...,\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0],\n",
              "        [  0,   0,   0]]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "real_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DYp5wENofGp"
      },
      "source": [
        "### Высчитывание метрик"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aLsq02p-xOcS"
      },
      "outputs": [],
      "source": [
        "KEYPOINTS = {\n",
        "    \"nose\": 0,\n",
        "    \"left eye\": 1,\n",
        "    \"right eye\": 2,\n",
        "    \"left ear\": 3,\n",
        "    \"right ear\": 4,\n",
        "    \"left shoulder\": 5,\n",
        "    \"right shoulder\": 6,\n",
        "    \"left elbow\": 7,\n",
        "    \"right elbow\": 8,\n",
        "    \"left wrist\": 9,\n",
        "    \"right wrist\": 10,\n",
        "    \"left hip\": 11,\n",
        "    \"right hip\": 12,\n",
        "    \"left knee\": 13,\n",
        "    \"right knee\": 14,\n",
        "    \"left ankle\": 15,\n",
        "    \"right ankle\": 16,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nf8vc0QR7c3_"
      },
      "outputs": [],
      "source": [
        "# Calculate metrics\n",
        "\n",
        "# Size calculations\n",
        "\n",
        "def euclidian_metric(a, b):\n",
        "    res = np.array(a) - np.array(b)\n",
        "    return np.sqrt(np.sum(res ** 2))\n",
        "\n",
        "def calc_diag(points):\n",
        "    x_min = min(points[0], points[2])\n",
        "    x_max = max(points[0], points[2])\n",
        "    y_min = min(points[1], points[3])\n",
        "    y_max = max(points[1], points[3])\n",
        "    return euclidian_metric([x_min, y_min], [x_max, y_max])\n",
        "\n",
        "def calc_height(points):\n",
        "    y_min = min(points[1], points[3])\n",
        "    y_max = max(points[1], points[3])\n",
        "    return y_max - y_min\n",
        "\n",
        "def calc_square(points):\n",
        "    x_min = min(points[0], points[2])\n",
        "    x_max = max(points[0], points[2])\n",
        "    y_min = min(points[1], points[3])\n",
        "    y_max = max(points[1], points[3])\n",
        "    return (x_max - x_min) * (y_max - y_min)\n",
        "\n",
        "# Calculate PCK and PDJ\n",
        "\n",
        "def calc_pck(pred, real, threshold, bbox):\n",
        "    height = calc_height(bbox)\n",
        "    sum = 0\n",
        "    for p, r in zip(pred, real):\n",
        "        dist = euclidian_metric(p[:2], r[:2])\n",
        "        if dist < threshold * height:\n",
        "            sum += 1\n",
        "    return sum / len(real)\n",
        "\n",
        "def calc_pdj(pred, real, threshold, bbox):\n",
        "    diag = calc_diag(bbox)\n",
        "    sum = 0\n",
        "    for p, r in zip(pred, real):\n",
        "        dist = euclidian_metric(p[:2], r[:2])\n",
        "        if dist < threshold * diag:\n",
        "            sum += 1\n",
        "    return sum / len(real)\n",
        "\n",
        "# Calculate AP and mAP\n",
        "\n",
        "KEYPOINTS_OKS = {\n",
        "    \"nose\": 0.026,\n",
        "    \"left eye\": 0.025,\n",
        "    \"right eye\": 0.025,\n",
        "    \"left ear\": 0.035,\n",
        "    \"right ear\": 0.035,\n",
        "    \"left shoulder\": 0.079,\n",
        "    \"right shoulder\": 0.079,\n",
        "    \"left elbow\": 0.072,\n",
        "    \"right elbow\": 0.072,\n",
        "    \"left wrist\": 0.062,\n",
        "    \"right wrist\": 0.062,\n",
        "    \"left hip\": 0.107,\n",
        "    \"right hip\": 0.107,\n",
        "    \"left knee\": 0.087,\n",
        "    \"right knee\": 0.087,\n",
        "    \"left ankle\": 0.089,\n",
        "    \"right ankle\": 0.089,\n",
        "}\n",
        "\n",
        "def calc_oks(pred, real, bbox):\n",
        "    square = calc_square(bbox)\n",
        "    sum = 0\n",
        "    for k, p, r in zip(KEYPOINTS_OKS.values(), pred, real):\n",
        "        dist = euclidian_metric(p[:2], r[:2])\n",
        "        if r[2] > 0:\n",
        "            sum += np.exp( - dist * dist / (2 * square * k * k))\n",
        "    \n",
        "    return sum / np.sum(np.array(real)[:,2] > 0)\n",
        "\n",
        "def calc_AP(threshold, oks):\n",
        "    n = len(oks)\n",
        "    corrects = [np.array(oks) > threshold]\n",
        "    recalls = []\n",
        "    precisions = []\n",
        "    right_n = np.sum(corrects)\n",
        "    for ind in range(n):\n",
        "        precisions.append(np.sum(corrects[:i]) / i)\n",
        "        recalls.append(np.sum(corrects[:i]) / right_n)\n",
        "\n",
        "    d = {}\n",
        "    d[1] = 0\n",
        "    for r, p in zip(recalls, precisions):\n",
        "        x_min = min(d.keys())\n",
        "        if ((r in d.keys()) and (d[r] < p)) or ((not r in d.keys()) and (d[x_min] < p)):\n",
        "            d[r] = p\n",
        "    d[0] = 1\n",
        "\n",
        "    step_size = 1 / n\n",
        "    sorted_keys = sorted(d.keys())\n",
        "    sum = 0\n",
        "    for r_min, r_max in zip(sorted_keys[:-1], sorted_keys[1:]):\n",
        "        sum += round((r_max - r_min) / step_size, 1) * d[r_max]\n",
        "\n",
        "    return (sum + 1) / (n + 1)\n",
        "\n",
        "def calc_mAP(oks):\n",
        "    APs = []\n",
        "    for threshold in np.arange(0.5, 1, 0.05):\n",
        "        APs.append(calc_AP(threshold, oks))\n",
        "    \n",
        "    return np.mean(APs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AYxbExI7c39"
      },
      "source": [
        "### Первая модель: BlazePose by MediaPipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGess-pO7c3-"
      },
      "outputs": [],
      "source": [
        "mp_pose = mp.solutions.pose\n",
        "mp_drawing = mp.solutions.drawing_utils \n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "\n",
        "# help(mp_pose.Pose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAAENSK7j8DK"
      },
      "outputs": [],
      "source": [
        "KEYPOINT_DICT = {\n",
        "    \"nose\": mp_pose.PoseLandmark.NOSE,\n",
        "    \"left eye\": mp_pose.PoseLandmark.LEFT_EYE,\n",
        "    \"right eye\": mp_pose.PoseLandmark.RIGHT_EYE,\n",
        "    \"left ear\": mp_pose.PoseLandmark.LEFT_EAR,\n",
        "    \"right ear\": mp_pose.PoseLandmark.RIGHT_EAR,\n",
        "    \"left shoulder\": mp_pose.PoseLandmark.LEFT_SHOULDER,\n",
        "    \"right shoulder\": mp_pose.PoseLandmark.RIGHT_SHOULDER,\n",
        "    \"left elbow\": mp_pose.PoseLandmark.LEFT_ELBOW,\n",
        "    \"right elbow\": mp_pose.PoseLandmark.RIGHT_ELBOW,\n",
        "    \"left wrist\": mp_pose.PoseLandmark.LEFT_WRIST,\n",
        "    \"right wrist\": mp_pose.PoseLandmark.RIGHT_WRIST,\n",
        "    \"left hip\": mp_pose.PoseLandmark.LEFT_HIP,\n",
        "    \"right hip\": mp_pose.PoseLandmark.RIGHT_HIP,\n",
        "    \"left knee\": mp_pose.PoseLandmark.LEFT_KNEE,\n",
        "    \"right knee\": mp_pose.PoseLandmark.RIGHT_KNEE,\n",
        "    \"left ankle\": mp_pose.PoseLandmark.LEFT_ANKLE,\n",
        "    \"right ankle\": mp_pose.PoseLandmark.RIGHT_ANKLE,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqoFiFuC7c3-",
        "scrolled": true,
        "outputId": "abea7fe1-0920-4667-be43-b11bbe1ff763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 0 batches.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [01:21<00:00, 12.23it/s]\n",
            "100%|██████████| 1000/1000 [01:21<00:00, 12.34it/s]\n",
            "100%|██████████| 1000/1000 [06:15<00:00,  2.67it/s]\n",
            "100%|██████████| 1000/1000 [06:13<00:00,  2.68it/s]\n",
            "100%|██████████| 1000/1000 [06:08<00:00,  2.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 5 batches.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [06:05<00:00,  2.73it/s]\n",
            "100%|██████████| 1000/1000 [06:14<00:00,  2.67it/s]\n",
            "100%|██████████| 1000/1000 [06:25<00:00,  2.60it/s]\n",
            "100%|██████████| 1000/1000 [06:23<00:00,  2.61it/s]\n",
            "100%|██████████| 1000/1000 [06:11<00:00,  2.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 10 batches.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [06:33<00:00,  2.54it/s]\n",
            "100%|██████████| 1000/1000 [06:25<00:00,  2.59it/s]\n",
            "100%|██████████| 1000/1000 [06:29<00:00,  2.57it/s]\n",
            "100%|██████████| 1000/1000 [06:28<00:00,  2.58it/s]\n",
            "100%|██████████| 1000/1000 [06:22<00:00,  2.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 15 batches.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [06:21<00:00,  2.62it/s]\n",
            "100%|██████████| 1000/1000 [06:20<00:00,  2.63it/s]\n",
            "100%|██████████| 1000/1000 [06:18<00:00,  2.64it/s]\n",
            "100%|██████████| 1000/1000 [06:21<00:00,  2.62it/s]\n",
            "100%|██████████| 1000/1000 [06:13<00:00,  2.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 20 batches.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [06:15<00:00,  2.66it/s]\n",
            "100%|██████████| 1000/1000 [06:17<00:00,  2.65it/s]\n",
            "100%|██████████| 1000/1000 [06:10<00:00,  2.70it/s]\n",
            "100%|██████████| 1000/1000 [06:28<00:00,  2.58it/s]\n",
            "100%|██████████| 1000/1000 [06:14<00:00,  2.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 25 batches.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [05:58<00:00,  2.79it/s]\n",
            "100%|██████████| 1000/1000 [06:12<00:00,  2.68it/s]\n",
            "100%|██████████| 1000/1000 [06:05<00:00,  2.73it/s]\n",
            "100%|██████████| 1000/1000 [06:15<00:00,  2.66it/s]\n",
            "100%|██████████| 1000/1000 [05:57<00:00,  2.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Errors: 9979 / 30000\n",
            "Percentage of errors: 33.263333333333335 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "time_landmarks = []\n",
        "errors = 0\n",
        "\n",
        "mp_real_data = []\n",
        "mp_bboxes = []\n",
        "\n",
        "with mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5, model_complexity=1) as pose:\n",
        "    for i in range(0, len(names), 1000):\n",
        "        if (i) % 5000 == 0:\n",
        "            print(f\"Processed {i // 1000} batches.\")\n",
        "\n",
        "        for ind, name in enumerate(tqdm(names[i : i + 1000])):\n",
        "            image = cv2.imread(PATH + 'images/' + name)\n",
        "\n",
        "            start = time.time()\n",
        "            keypoints = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "            end = time.time()\n",
        "\n",
        "            image_height, image_width, _ = image.shape\n",
        "            if not keypoints.pose_landmarks:\n",
        "                errors += 1\n",
        "                continue\n",
        "\n",
        "            coords = []\n",
        "            for key in KEYPOINTS.keys():\n",
        "                coords.append([\n",
        "                        keypoints.pose_landmarks.landmark[KEYPOINT_DICT[key]].x * image_width,\n",
        "                        keypoints.pose_landmarks.landmark[KEYPOINT_DICT[key]].y * image_height,\n",
        "                        keypoints.pose_landmarks.landmark[KEYPOINT_DICT[key]].visibility\n",
        "                ])\n",
        "            mp_real_data.append(real_data[i + ind])\n",
        "            mp_bboxes.append(bboxes[i + ind])\n",
        "            results.append(np.array(coords))\n",
        "            time_landmarks.append(end - start)  \n",
        "print(f\"\\n\\nErrors: {errors} / {len(names)}\\nPercentage of errors: {errors * 100 / len(names)} %\")    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZJVd7UQoR0Q",
        "outputId": "0f6d35a7-4c1d-42b3-dcf9-009bdcb00d29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:78: RuntimeWarning: invalid value encountered in long_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:76: RuntimeWarning: divide by zero encountered in double_scalars\n"
          ]
        }
      ],
      "source": [
        "# calculate metrics\n",
        "\n",
        "pck = {}\n",
        "pdj = {}\n",
        "\n",
        "for threshold in [0.05, 0.2, 0.5]:\n",
        "    pdjs = []\n",
        "    pcks = []\n",
        "    for pred, real, bbox in zip(results, mp_real_data, mp_bboxes):\n",
        "        pdjs.append(calc_pdj(pred, real, threshold, bbox))\n",
        "        pcks.append(calc_pck(pred, real, threshold, bbox))\n",
        "    \n",
        "    pck[threshold] = round(np.mean(pcks), 3)\n",
        "    pdj[threshold] = round(np.mean(pdjs), 3)\n",
        "\n",
        "oks = []\n",
        "for pred, real, bbox in zip(results, mp_real_data, mp_bboxes):\n",
        "    oks.append(calc_oks(pred, real, bbox))\n",
        "\n",
        "AP = {}\n",
        "for threshold in [0.5, 0.75]:\n",
        "    AP[threshold] = round(calc_AP(threshold, oks), 3)\n",
        "\n",
        "mAP = round(calc_mAP(oks), 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9sA6inqoR0R",
        "outputId": "d9903ab1-7cb1-4f5b-8469-f0f2fc7a2901"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pck: {0.05: 0.365, 0.2: 0.519, 0.5: 0.595}\n",
            "pdj: {0.05: 0.417, 0.2: 0.554, 0.5: 0.638}\n",
            "AP:  {0.5: 0.33, 0.75: 0.158}\n",
            "mAP: 0.173\n",
            "Average time: 0.081\n"
          ]
        }
      ],
      "source": [
        "print(f\"pck: {pck}\")\n",
        "print(f\"pdj: {pdj}\")\n",
        "print(f\"AP:  {AP}\")\n",
        "print(f\"mAP: {mAP}\")\n",
        "print(f\"Average time: {round(np.mean(time_landmarks), 3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCDHouZz7c3_"
      },
      "source": [
        "### Вторая модель: Move_Net.Singe_Pose.lightning by TensorFlowHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYYvqyBl7c3_"
      },
      "outputs": [],
      "source": [
        "model_name = \"movenet_lightning\"\n",
        "module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "\n",
        "def movenet(input_image):\n",
        "    \"\"\"Runs detection on an input image.\n",
        "\n",
        "    Args:\n",
        "        input_image: A [1, height, width, 3] tensor represents the input image\n",
        "        pixels. Note that the height/width should already be resized and match the\n",
        "        expected input resolution of the model before passing into this function.\n",
        "\n",
        "    Returns:\n",
        "        A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
        "        coordinates and scores.\n",
        "    \"\"\"\n",
        "    model = module.signatures['serving_default']\n",
        "\n",
        "    # SavedModel format expects tensor type of int32.\n",
        "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
        "    # Run model inference.\n",
        "    outputs = model(input_image)\n",
        "    # Output is a [1, 1, 17, 3] tensor.\n",
        "    keypoints_with_scores = outputs['output_0'].numpy()\n",
        "    return keypoints_with_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkyPcZ89wBON"
      },
      "outputs": [],
      "source": [
        "KEYPOINT_DICT = {\n",
        "    'nose': 0,\n",
        "    'left eye': 1,\n",
        "    'right eye': 2,\n",
        "    'left ear': 3,\n",
        "    'right ear': 4,\n",
        "    'left shoulder': 5,\n",
        "    'right shoulder': 6,\n",
        "    'left elbow': 7,\n",
        "    'right elbow': 8,\n",
        "    'left wrist': 9,\n",
        "    'right wrist': 10,\n",
        "    'left hip': 11,\n",
        "    'right hip': 12,\n",
        "    'left knee': 13,\n",
        "    'right knee': 14,\n",
        "    'left ankle': 15,\n",
        "    'right ankle': 16\n",
        "}\n",
        "\n",
        "# Maps bones to a matplotlib color name.\n",
        "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
        "    (0, 1): 'm',\n",
        "    (0, 2): 'c',\n",
        "    (1, 3): 'm',\n",
        "    (2, 4): 'c',\n",
        "    (0, 5): 'm',\n",
        "    (0, 6): 'c',\n",
        "    (5, 7): 'm',\n",
        "    (7, 9): 'm',\n",
        "    (6, 8): 'c',\n",
        "    (8, 10): 'c',\n",
        "    (5, 6): 'y',\n",
        "    (5, 11): 'm',\n",
        "    (6, 12): 'c',\n",
        "    (11, 12): 'y',\n",
        "    (11, 13): 'm',\n",
        "    (13, 15): 'm',\n",
        "    (12, 14): 'c',\n",
        "    (14, 16): 'c'\n",
        "}\n",
        "\n",
        "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
        "                                     height,\n",
        "                                     width,\n",
        "                                     keypoint_threshold=0.11):\n",
        "  \"\"\"Returns high confidence keypoints and edges for visualization.\n",
        "\n",
        "  Args:\n",
        "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
        "      the keypoint coordinates and scores returned from the MoveNet model.\n",
        "    height: height of the image in pixels.\n",
        "    width: width of the image in pixels.\n",
        "    keypoint_threshold: minimum confidence score for a keypoint to be\n",
        "      visualized.\n",
        "\n",
        "  Returns:\n",
        "    A (keypoints_xy, edges_xy, edge_colors) containing:\n",
        "      * the coordinates of all keypoints of all detected entities;\n",
        "      * the coordinates of all skeleton edges of all detected entities;\n",
        "      * the colors in which the edges should be plotted.\n",
        "  \"\"\"\n",
        "  keypoints_all = []\n",
        "  keypoint_edges_all = []\n",
        "  edge_colors = []\n",
        "  num_instances, _, _, _ = keypoints_with_scores.shape\n",
        "  for idx in range(num_instances):\n",
        "    kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
        "    kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
        "    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
        "    kpts_absolute_xy = np.stack(\n",
        "        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
        "    kpts_above_thresh_absolute = kpts_absolute_xy[\n",
        "        kpts_scores > keypoint_threshold, :]\n",
        "    keypoints_all.append(kpts_above_thresh_absolute)\n",
        "\n",
        "    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
        "      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
        "          kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
        "        x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
        "        y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
        "        x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
        "        y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
        "        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
        "        keypoint_edges_all.append(line_seg)\n",
        "        edge_colors.append(color)\n",
        "  if keypoints_all:\n",
        "    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
        "  else:\n",
        "    keypoints_xy = np.zeros((0, 17, 2))\n",
        "\n",
        "  if keypoint_edges_all:\n",
        "    edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
        "  else:\n",
        "    edges_xy = np.zeros((0, 2, 2))\n",
        "  return keypoints_xy, edges_xy, edge_colors\n",
        "\n",
        "\n",
        "def draw_prediction_on_image(\n",
        "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
        "    output_image_height=None):\n",
        "  \"\"\"Draws the keypoint predictions on image.\n",
        "\n",
        "  Args:\n",
        "    image: A numpy array with shape [height, width, channel] representing the\n",
        "      pixel values of the input image.\n",
        "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
        "      the keypoint coordinates and scores returned from the MoveNet model.\n",
        "    crop_region: A dictionary that defines the coordinates of the bounding box\n",
        "      of the crop region in normalized coordinates (see the init_crop_region\n",
        "      function below for more detail). If provided, this function will also\n",
        "      draw the bounding box on the image.\n",
        "    output_image_height: An integer indicating the height of the output image.\n",
        "      Note that the image aspect ratio will be the same as the input image.\n",
        "\n",
        "  Returns:\n",
        "    A numpy array with shape [out_height, out_width, channel] representing the\n",
        "    image overlaid with keypoint predictions.\n",
        "  \"\"\"\n",
        "  height, width, channel = image.shape\n",
        "  aspect_ratio = float(width) / height\n",
        "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
        "  # To remove the huge white borders\n",
        "  fig.tight_layout(pad=0)\n",
        "  ax.margins(0)\n",
        "  ax.set_yticklabels([])\n",
        "  ax.set_xticklabels([])\n",
        "  plt.axis('off')\n",
        "\n",
        "  im = ax.imshow(image)\n",
        "  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
        "  ax.add_collection(line_segments)\n",
        "  # Turn off tick labels\n",
        "  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
        "\n",
        "  (keypoint_locs, keypoint_edges,\n",
        "   edge_colors) = _keypoints_and_edges_for_display(\n",
        "       keypoints_with_scores, height, width)\n",
        "\n",
        "  line_segments.set_segments(keypoint_edges)\n",
        "  line_segments.set_color(edge_colors)\n",
        "  if keypoint_edges.shape[0]:\n",
        "    line_segments.set_segments(keypoint_edges)\n",
        "    line_segments.set_color(edge_colors)\n",
        "  if keypoint_locs.shape[0]:\n",
        "    scat.set_offsets(keypoint_locs)\n",
        "\n",
        "  if crop_region is not None:\n",
        "    xmin = max(crop_region['x_min'] * width, 0.0)\n",
        "    ymin = max(crop_region['y_min'] * height, 0.0)\n",
        "    rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
        "    rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
        "    rect = patches.Rectangle(\n",
        "        (xmin,ymin),rec_width,rec_height,\n",
        "        linewidth=1,edgecolor='b',facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "  fig.canvas.draw()\n",
        "  image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "  image_from_plot = image_from_plot.reshape(\n",
        "      fig.canvas.get_width_height()[::-1] + (3,))\n",
        "  plt.close(fig)\n",
        "  if output_image_height is not None:\n",
        "    output_image_width = int(output_image_height / height * width)\n",
        "    image_from_plot = cv2.resize(\n",
        "        image_from_plot, dsize=(output_image_width, output_image_height),\n",
        "         interpolation=cv2.INTER_CUBIC)\n",
        "  return image_from_plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJItJMjIuHn1",
        "outputId": "2fe475e2-6529-46f7-c7f3-b79018c6c5a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 0 batches.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:48<00:00, 20.58it/s]\n",
            "100%|██████████| 1000/1000 [00:45<00:00, 21.90it/s]\n",
            "100%|██████████| 1000/1000 [00:46<00:00, 21.48it/s]\n",
            "100%|██████████| 1000/1000 [00:51<00:00, 19.55it/s]\n",
            "100%|██████████| 1000/1000 [00:47<00:00, 20.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 5 batches.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:47<00:00, 21.15it/s]\n",
            "100%|██████████| 1000/1000 [00:48<00:00, 20.78it/s]\n",
            "100%|██████████| 1000/1000 [00:46<00:00, 21.38it/s]\n",
            "100%|██████████| 1000/1000 [00:48<00:00, 20.81it/s]\n",
            "100%|██████████| 1000/1000 [00:47<00:00, 21.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 10 batches.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:47<00:00, 20.87it/s]\n",
            "100%|██████████| 1000/1000 [00:47<00:00, 21.27it/s]\n",
            "100%|██████████| 1000/1000 [00:48<00:00, 20.81it/s]\n",
            "100%|██████████| 1000/1000 [00:46<00:00, 21.34it/s]\n",
            "100%|██████████| 1000/1000 [00:46<00:00, 21.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 15 batches.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:46<00:00, 21.69it/s]\n",
            "100%|██████████| 1000/1000 [00:51<00:00, 19.47it/s]\n",
            "100%|██████████| 1000/1000 [00:47<00:00, 21.04it/s]\n",
            "100%|██████████| 1000/1000 [00:46<00:00, 21.43it/s]\n",
            "100%|██████████| 1000/1000 [00:46<00:00, 21.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 20 batches.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:46<00:00, 21.42it/s]\n",
            "100%|██████████| 1000/1000 [00:46<00:00, 21.34it/s]\n",
            "100%|██████████| 1000/1000 [00:47<00:00, 21.26it/s]\n",
            "100%|██████████| 1000/1000 [00:48<00:00, 20.59it/s]\n",
            "100%|██████████| 1000/1000 [00:50<00:00, 19.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 25 batches.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:47<00:00, 20.92it/s]\n",
            "100%|██████████| 1000/1000 [00:48<00:00, 20.80it/s]\n",
            "100%|██████████| 1000/1000 [00:48<00:00, 20.75it/s]\n",
            "100%|██████████| 1000/1000 [00:52<00:00, 19.07it/s]\n",
            "100%|██████████| 1000/1000 [00:48<00:00, 20.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Errors: 307 / 30000\n",
            "Percentage of errors: 1.0233333333333334 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "input_size = 192\n",
        "results = []\n",
        "time_landmarks = []\n",
        "errors = 0\n",
        "\n",
        "mn_real_data = []\n",
        "mn_bboxes = []\n",
        "\n",
        "for i in range(0, len(names), 1000):\n",
        "    if i % 5000 == 0:\n",
        "        print(f\"Processed {i // 1000} batches.\")\n",
        "\n",
        "    for ind, name in enumerate(tqdm(names[i : i + 1000])):\n",
        "        image = tf.image.decode_jpeg(tf.io.read_file(PATH + \"images/\" + name))\n",
        "\n",
        "        # Resize and pad the image to keep the aspect ratio and fit the expected size.\n",
        "        input_image = tf.expand_dims(image, axis=0)\n",
        "        input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
        "        image_height, image_width, _ = image.shape\n",
        "\n",
        "        # Run model inference.\n",
        "        try:\n",
        "            start = time.time()\n",
        "            keypoints_with_scores = movenet(input_image)\n",
        "            end = time.time()\n",
        "\n",
        "            mn_real_data.append(real_data[i + ind])\n",
        "            mn_bboxes.append(bboxes[i + ind])\n",
        "        except:\n",
        "            errors += 1\n",
        "\n",
        "        coords = []\n",
        "        for key in KEYPOINTS.keys():\n",
        "            coord = keypoints_with_scores[0,0,KEYPOINT_DICT[key]] * [image_height, image_width, 1]\n",
        "            coords.append([coord[1], coord[0], coord[2]])\n",
        "\n",
        "        results.append(np.array(coords))\n",
        "        time_landmarks.append(end - start)\n",
        "    \n",
        "print(f\"\\nErrors: {errors} / {len(names)}\\nPercentage of errors: {errors * 100 / len(names)} %\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo5e_fEP9Lde",
        "outputId": "518548e8-d163-430e-ea18-8f3afaa60f73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:78: RuntimeWarning: invalid value encountered in long_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:76: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:88: RuntimeWarning: invalid value encountered in long_scalars\n"
          ]
        }
      ],
      "source": [
        "# calculate metrics\n",
        "\n",
        "pck = {}\n",
        "pdj = {}\n",
        "\n",
        "for threshold in [0.05, 0.2, 0.5]:\n",
        "    pdjs = []\n",
        "    pcks = []\n",
        "    for pred, real, bbox in zip(results, mn_real_data, mn_bboxes):\n",
        "        pdjs.append(calc_pdj(pred, real, threshold, bbox))\n",
        "        pcks.append(calc_pck(pred, real, threshold, bbox))\n",
        "    \n",
        "    pck[threshold] = round(np.mean(pcks), 3)\n",
        "    pdj[threshold] = round(np.mean(pdjs), 3)\n",
        "\n",
        "oks = []\n",
        "for pred, real, bbox in zip(results, mn_real_data, mn_bboxes):\n",
        "    oks.append(calc_oks(pred, real, bbox))\n",
        "\n",
        "AP = {}\n",
        "for threshold in [0.5, 0.75]:\n",
        "    AP[threshold] = round(calc_AP(threshold, oks), 3)\n",
        "\n",
        "mAP = round(calc_mAP(oks), 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTKzTuGo9Ldf",
        "outputId": "cfa78894-b1a8-49f5-b6b0-8e2aee2b0ab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pck: {0.05: 0.004, 0.2: 0.052, 0.5: 0.223}\n",
            "pdj: {0.05: 0.006, 0.2: 0.078, 0.5: 0.32}\n",
            "AP:  {0.5: 0.0, 0.75: 0.0}\n",
            "mAP: 0.0\n",
            "Average time: 0.035\n"
          ]
        }
      ],
      "source": [
        "print(f\"pck: {pck}\")\n",
        "print(f\"pdj: {pdj}\")\n",
        "print(f\"AP:  {AP}\")\n",
        "print(f\"mAP: {mAP}\")\n",
        "print(f\"Average time: {round(np.mean(time_landmarks), 3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4sVS__qw45U"
      },
      "source": [
        "### Третья модель: OpenPose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZQbISyZ7c4A"
      },
      "outputs": [],
      "source": [
        "from os.path import exists, join, basename, splitext\n",
        "\n",
        "git_repo_url = 'https://github.com/CMU-Perceptual-Computing-Lab/openpose.git'\n",
        "project_name = splitext(basename(git_repo_url))[0]\n",
        "if not exists(project_name):\n",
        "    # see: https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/949\n",
        "    # install new CMake because of CUDA10\n",
        "    !wget -q https://cmake.org/files/v3.13/cmake-3.13.0-Linux-x86_64.tar.gz\n",
        "    !tar xfz cmake-3.13.0-Linux-x86_64.tar.gz --strip-components=1 -C /usr/local\n",
        "    # clone openpose\n",
        "    !git clone -q --depth 1 $git_repo_url\n",
        "    !sed -i 's/execute_process(COMMAND git checkout master WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\\/3rdparty\\/caffe)/execute_process(COMMAND git checkout f019d0dfe86f49d1140961f8c7dec22130c83154 WORKING_DIRECTORY ${CMAKE_SOURCE_DIR}\\/3rdparty\\/caffe)/g' openpose/CMakeLists.txt\n",
        "    # install system dependencies\n",
        "    !apt-get -qq install -y libatlas-base-dev \n",
        "    !apt-get -qq install -y libprotobuf-dev \n",
        "    !apt-get -qq install -y libleveldb-dev \n",
        "    !apt-get -qq install -y libsnappy-dev\n",
        "    !apt-get -qq install -y libhdf5-serial-dev\n",
        "    !apt-get -qq install -y protobuf-compiler\n",
        "    !apt-get -qq install -y libgflags-dev\n",
        "    !apt-get -qq install -y libgoogle-glog-dev\n",
        "    # build openpose\n",
        "    !cd openpose && rm -rf build || true && mkdir build && cd build && cmake -DBUILD_PYTHON=ON .. && make -j`nproc`\n",
        "if not exists(\"openpose/images\"):\n",
        "    !mkdir openpose/images\n",
        "%cd openpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qB28OxOsEeL"
      },
      "outputs": [],
      "source": [
        "KEYPOINT_DICT = {\n",
        "    \"nose\": 0,\n",
        "    \"left eye\": 16,\n",
        "    \"right eye\": 15,\n",
        "    \"left ear\": 18,\n",
        "    \"right ear\": 17,\n",
        "    \"left shoulder\": 5,\n",
        "    \"right shoulder\": 2,\n",
        "    \"left elbow\": 6,\n",
        "    \"right elbow\": 3,\n",
        "    \"left wrist\": 7,\n",
        "    \"right wrist\": 4,\n",
        "    \"left hip\": 12,\n",
        "    \"right hip\": 9,\n",
        "    \"left knee\": 13,\n",
        "    \"right knee\": 10,\n",
        "    \"left ankle\": 14,\n",
        "    \"right ankle\": 11,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLkrIGhR7c4A",
        "outputId": "a8b26d62-9bd2-4666-c861-94f4db8726bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 0 batches.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [01:05<00:00, 15.25it/s]\n",
            "100%|██████████| 1000/1000 [01:04<00:00, 15.58it/s]\n",
            "100%|██████████| 1000/1000 [05:31<00:00,  3.02it/s]\n",
            "100%|██████████| 1000/1000 [05:30<00:00,  3.03it/s]\n",
            "100%|██████████| 1000/1000 [05:32<00:00,  3.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 5 batches.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [05:31<00:00,  3.02it/s]\n",
            "100%|██████████| 1000/1000 [05:30<00:00,  3.02it/s]\n",
            "100%|██████████| 1000/1000 [05:45<00:00,  2.90it/s]\n",
            "100%|██████████| 1000/1000 [05:34<00:00,  2.99it/s]\n",
            "100%|██████████| 1000/1000 [05:33<00:00,  3.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 10 batches.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [05:28<00:00,  3.04it/s]\n",
            "100%|██████████| 1000/1000 [05:27<00:00,  3.05it/s]\n",
            "100%|██████████| 1000/1000 [05:26<00:00,  3.07it/s]\n",
            "100%|██████████| 1000/1000 [05:28<00:00,  3.04it/s]\n",
            "100%|██████████| 1000/1000 [05:28<00:00,  3.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 15 batches.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [05:22<00:00,  3.10it/s]\n",
            "100%|██████████| 1000/1000 [05:29<00:00,  3.04it/s]\n",
            "100%|██████████| 1000/1000 [05:24<00:00,  3.08it/s]\n",
            "100%|██████████| 1000/1000 [05:25<00:00,  3.07it/s]\n",
            "100%|██████████| 1000/1000 [05:26<00:00,  3.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 20 batches.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [05:28<00:00,  3.05it/s]\n",
            "100%|██████████| 1000/1000 [05:27<00:00,  3.05it/s]\n",
            "100%|██████████| 1000/1000 [05:25<00:00,  3.08it/s]\n",
            "100%|██████████| 1000/1000 [05:26<00:00,  3.06it/s]\n",
            "100%|██████████| 1000/1000 [05:25<00:00,  3.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 25 batches.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [05:26<00:00,  3.06it/s]\n",
            "100%|██████████| 1000/1000 [05:33<00:00,  3.00it/s]\n",
            "100%|██████████| 1000/1000 [05:31<00:00,  3.02it/s]\n",
            "100%|██████████| 1000/1000 [05:26<00:00,  3.06it/s]\n",
            "100%|██████████| 1000/1000 [05:28<00:00,  3.05it/s]\n"
          ]
        }
      ],
      "source": [
        "# Import general libraries\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "OpenposeDir = '/content/openpose/'\n",
        "\n",
        "# Set Python Openpose Directory for python api (Important)\n",
        "pyopenpose_dir = os.path.join(OpenposeDir,'build','python') # ex: '/content/openpose/build/python'\n",
        "if pyopenpose_dir not in sys.path:\n",
        "    sys.path.append(pyopenpose_dir)\n",
        "from openpose import pyopenpose as op\n",
        "\n",
        "# Custom Params (refer to openpose/include/openpose/flags.hpp for more parameters)\n",
        "params = dict()\n",
        "params[\"model_folder\"] = os.path.join(OpenposeDir,'models')  # ex: '/content/openpose/models'\n",
        "\n",
        "# Starting OpenPose\n",
        "opWrapper = op.WrapperPython()\n",
        "opWrapper.configure(params)\n",
        "opWrapper.start()\n",
        "\n",
        "results = []\n",
        "time_landmarks = []\n",
        "# Process Image\n",
        "for i in range(0, len(names), 1000):\n",
        "    if i % 5000 == 0:\n",
        "        print(f\"Processed {i // 1000} batches.\")\n",
        "        \n",
        "    for name in tqdm(names[i : i + 1000]):\n",
        "        input_image = cv2.imread((PATH + \"images/\" + name))\n",
        "        start = time.time()\n",
        "        datum = op.Datum()\n",
        "        datum.cvInputData = input_image\n",
        "        opWrapper.emplaceAndPop(op.VectorDatum([datum]))\n",
        "        network_output = datum.poseKeypoints\n",
        "        end = time.time()\n",
        "\n",
        "        coords = []\n",
        "        if not network_output is None:\n",
        "            for key in KEYPOINTS.keys():\n",
        "                sec_index = KEYPOINT_DICT[key]\n",
        "                coords.append(network_output[0, sec_index])\n",
        "        else:\n",
        "            coords = np.zeros((14,3), dtype=np.float32)\n",
        "\n",
        "        results.append(np.array(coords))\n",
        "        time_landmarks.append(end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lGWvj7NQVgu",
        "outputId": "0e604c41-4160-462e-a4f9-135c255321af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:78: RuntimeWarning: invalid value encountered in long_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:76: RuntimeWarning: divide by zero encountered in double_scalars\n"
          ]
        }
      ],
      "source": [
        "# calculate metrics\n",
        "\n",
        "pck = {}\n",
        "pdj = {}\n",
        "\n",
        "for threshold in [0.05, 0.2, 0.5]:\n",
        "    pdjs = []\n",
        "    pcks = []\n",
        "    for pred, real, bbox in zip(results, real_data, bboxes):\n",
        "        pdjs.append(calc_pdj(pred, real, threshold, bbox))\n",
        "        pcks.append(calc_pck(pred, real, threshold, bbox))\n",
        "    \n",
        "    pck[threshold] = round(np.mean(pcks), 3)\n",
        "    pdj[threshold] = round(np.mean(pdjs), 3)\n",
        "\n",
        "oks = []\n",
        "for pred, real, bbox in zip(results, real_data, bboxes):\n",
        "    oks.append(calc_oks(pred, real, bbox))\n",
        "\n",
        "AP = {}\n",
        "for threshold in [0.5, 0.75]:\n",
        "    AP[threshold] = round(calc_AP(threshold, oks), 3)\n",
        "\n",
        "mAP = round(calc_mAP(oks), 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-P3NY-KQVg7",
        "outputId": "af02a837-6309-444f-81d3-ae7381902752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pck: {0.05: 0.575, 0.2: 0.638, 0.5: 0.686}\n",
            "pdj: {0.05: 0.613, 0.2: 0.658, 0.5: 0.72}\n",
            "AP:  {0.5: 0.562, 0.75: 0.398}\n",
            "mAP: 0.384\n",
            "Average time: 0.08\n"
          ]
        }
      ],
      "source": [
        "print(f\"pck: {pck}\")\n",
        "print(f\"pdj: {pdj}\")\n",
        "print(f\"AP:  {AP}\")\n",
        "print(f\"mAP: {mAP}\")\n",
        "print(f\"Average time: {round(np.mean(time_landmarks), 3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrEE8b5yP8oG"
      },
      "source": [
        "### Четвертая модель: MMPose by Open-MMLab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dvKWH89zw0x",
        "outputId": "ba166cdf-d61a-43a7-8358-56a757cc4bea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n",
            "gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
            "Copyright (C) 2017 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n",
            "/usr/local/bin/python\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# check NVCC version\n",
        "!nvcc -V\n",
        "\n",
        "# check GCC version\n",
        "!gcc --version\n",
        "\n",
        "# check python in conda environment\n",
        "!which python\n",
        "\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVMUH4oy7c4A"
      },
      "outputs": [],
      "source": [
        "# install dependencies: (use cu111 because colab has CUDA 11.1)\n",
        "%pip install torch==1.10.0+cu111 torchvision==0.11.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# install mmcv-full thus we could use CUDA operators\n",
        "%pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.10.0/index.html\n",
        "\n",
        "# install mmdet for inference demo\n",
        "%pip install mmdet\n",
        "\n",
        "# clone mmpose repo\n",
        "%rm -rf mmpose\n",
        "!git clone https://github.com/open-mmlab/mmpose.git\n",
        "%cd mmpose\n",
        "\n",
        "# install mmpose dependencies\n",
        "%pip install -r requirements.txt\n",
        "\n",
        "# install mmpose in develop mode\n",
        "%pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqteUccHgU1N",
        "outputId": "12548a85-d198-4e5b-d010-e945c9d8bc0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 1.10.0+cu111 True\n",
            "torchvision version: 0.11.0+cu111\n",
            "mmpose version: 0.27.0\n",
            "cuda version: 11.1\n",
            "compiler information: GCC 7.3\n"
          ]
        }
      ],
      "source": [
        "# Check Pytorch installation\n",
        "import torch, torchvision\n",
        "\n",
        "print('torch version:', torch.__version__, torch.cuda.is_available())\n",
        "print('torchvision version:', torchvision.__version__)\n",
        "\n",
        "# Check MMPose installation\n",
        "import mmpose\n",
        "\n",
        "print('mmpose version:', mmpose.__version__)\n",
        "\n",
        "# Check mmcv installation\n",
        "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
        "\n",
        "print('cuda version:', get_compiling_cuda_version())\n",
        "print('compiler information:', get_compiler_version())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "c66c9cdcfd9c4dad818afd2a40177730",
            "25a1a3f7d87f42ca9997bd8b494d3bce",
            "cdea2a18b73846da9953360235e5c249",
            "1942c3645ada4837ba8ce7a79da9a495",
            "6ebbd517f95f4d86ab78c00d641ce543",
            "52d938e22bdd4b86afd1175ba7dd81d4",
            "c9b167db2dee48b0a06fc2bebdb4ec1b",
            "f0992600298f44c8b91ecf451bf24c06",
            "f1fe975e7e8f455c823a3c9af5bf3ea5",
            "62b2f20a1e2742f19b87f84911f74624",
            "fafa7d2f44d444b7a862246940b074a7",
            "70b6916e2daf4021a23f7a2be0e3a387",
            "554a901e2c4d4b7a816cc65fd88296c1",
            "9b2847c8ec65472680993120a817f3c7",
            "d71d138996cd4923acb1c26b109bcd10",
            "b19c4df8a201400fa40ef704f804f275",
            "9f92ccf869fc428085042f9299cadd66",
            "b7e0de122acc432a82f8ec2e40673aba",
            "b5f98e1dd49940cfa0f444db85caee64",
            "687cefb903784eddbf2582a37770a950",
            "4f9648b87d6443018a7ddafc1e0312bf",
            "b6618af70aa141b9a6f26d947693b2f1"
          ]
        },
        "id": "AaUNCi28zw0z",
        "outputId": "9317304e-c270-44d2-fed8-07a210bf5f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth\" to /root/.cache/torch/hub/checkpoints/hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/243M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c66c9cdcfd9c4dad818afd2a40177730"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load checkpoint from http path: https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\" to /root/.cache/torch/hub/checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/160M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70b6916e2daf4021a23f7a2be0e3a387"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from mmpose.apis import (inference_top_down_pose_model, init_pose_model,\n",
        "                         vis_pose_result, process_mmdet_results)\n",
        "from mmdet.apis import inference_detector, init_detector\n",
        "\n",
        "pose_config = 'configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/hrnet_w48_coco_256x192.py'\n",
        "pose_checkpoint = 'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth'\n",
        "det_config = 'demo/mmdetection_cfg/faster_rcnn_r50_fpn_coco.py'\n",
        "det_checkpoint = 'https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\n",
        "\n",
        "# initialize pose model\n",
        "pose_model = init_pose_model(pose_config, pose_checkpoint)\n",
        "# initialize detector\n",
        "det_model = init_detector(det_config, det_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "I0bfJ1AO8Kb2"
      },
      "outputs": [],
      "source": [
        "KEYPOINT_DICT = {\n",
        "    \"nose\": 0,\n",
        "    \"left eye\": 1,\n",
        "    \"right eye\": 2,\n",
        "    \"left ear\": 3,\n",
        "    \"right ear\": 4,\n",
        "    \"left shoulder\": 5,\n",
        "    \"right shoulder\": 6,\n",
        "    \"left elbow\": 7,\n",
        "    \"right elbow\": 8,\n",
        "    \"left wrist\": 9,\n",
        "    \"right wrist\": 10,\n",
        "    \"left hip\": 11,\n",
        "    \"right hip\": 12,\n",
        "    \"left knee\": 13,\n",
        "    \"right knee\": 14,\n",
        "    \"left ankle\": 15,\n",
        "    \"right ankle\": 16,\n",
        "}\n",
        "\n",
        "# \"keypoints\": [\n",
        "#             \"nose\",\"left_eye\",\"right_eye\",\"left_ear\",\"right_ear\",\n",
        "#             \"left_shoulder\",\"right_shoulder\",\"left_elbow\",\"right_elbow\",\n",
        "#             \"left_wrist\",\"right_wrist\",\"left_hip\",\"right_hip\",\n",
        "#             \"left_knee\",\"right_knee\",\"left_ankle\",\"right_ankle\"\n",
        "#         ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gcsk6DcRwBR4",
        "outputId": "072a8dc0-b89c-4bda-c6f0-e01e3bea10cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 25 batches.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [12:42<00:00,  1.31it/s]\n",
            "100%|██████████| 1000/1000 [12:41<00:00,  1.31it/s]\n",
            "100%|██████████| 1000/1000 [12:20<00:00,  1.35it/s]\n",
            "100%|██████████| 1000/1000 [12:09<00:00,  1.37it/s]\n",
            "100%|██████████| 1000/1000 [12:18<00:00,  1.35it/s]\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "time_landmarks = []\n",
        "\n",
        "# Эта модель не может за один раз обработать сразу 30 тысяч фотографий \n",
        "# из-за ограничений на использование Google Colab.\n",
        "\n",
        "# Получение будет построено в несколько этапов \n",
        "# с сохранением промежуточных результатов в файл\n",
        "\n",
        "start_index = 25_000\n",
        "end_index = 30_000\n",
        "\n",
        "for i in range(start_index, end_index, 1000):\n",
        "    if i % 5000 == 0:\n",
        "        print(f\"Processed {i // 1000} batches.\")\n",
        "        \n",
        "    for name in tqdm(names[i : i + 1000]):\n",
        "        name = PATH + 'images/' + name\n",
        "        \n",
        "        start = time.time()\n",
        "        mmdet_results = inference_detector(det_model, name)\n",
        "        person_results = process_mmdet_results(mmdet_results, cat_id=1)\n",
        "        pose_results, returned_outputs = inference_top_down_pose_model(\n",
        "            pose_model,\n",
        "            name,\n",
        "            person_results,\n",
        "            bbox_thr=0.3,\n",
        "            format='xyxy',\n",
        "            dataset=pose_model.cfg.data.test.type)\n",
        "        end = time.time()\n",
        "        \n",
        "        if pose_results:\n",
        "            pose_results = pose_results[0]['keypoints']\n",
        "        else:\n",
        "            pose_results = np.zeros((17,3))\n",
        "\n",
        "        coords = []\n",
        "        for key in KEYPOINTS.keys():\n",
        "            coords.append(pose_results[KEYPOINT_DICT[key]])\n",
        "\n",
        "        results.append(np.array(coords))\n",
        "        time_landmarks.append(end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw7OMkgOCLWe",
        "outputId": "da9f042a-c4ca-4b1a-d3e9-a36bf30e2b22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "len(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ddxk-U6xT03T"
      },
      "outputs": [],
      "source": [
        "filename = \"results_.npy\"\n",
        "if os.path.exists(filename):\n",
        "    res = np.load(filename)\n",
        "    results = [*res, *results]\n",
        "\n",
        "np.save(filename, results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD3tPEe0RCME",
        "outputId": "a62fb1ad-c5fb-46bc-d90b-10a73809ac00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:78: RuntimeWarning: invalid value encountered in long_scalars\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:76: RuntimeWarning: divide by zero encountered in double_scalars\n"
          ]
        }
      ],
      "source": [
        "# calculate metrics\n",
        "\n",
        "pck = {}\n",
        "pdj = {}\n",
        "\n",
        "for threshold in [0.05, 0.2, 0.5]:\n",
        "    pdjs = []\n",
        "    pcks = []\n",
        "    for pred, real, bbox in zip(results, real_data[:len(results)], bboxes[:len(results)]):\n",
        "        pdjs.append(calc_pdj(pred, real, threshold, bbox))\n",
        "        pcks.append(calc_pck(pred, real, threshold, bbox))\n",
        "    \n",
        "    pck[threshold] = round(np.mean(pcks), 3)\n",
        "    pdj[threshold] = round(np.mean(pdjs), 3)\n",
        "\n",
        "oks = []\n",
        "for pred, real, bbox in zip(results, real_data[:len(results)], bboxes[:len(results)]):\n",
        "    oks.append(calc_oks(pred, real, bbox))\n",
        "\n",
        "AP = {}\n",
        "for threshold in [0.5, 0.75]:\n",
        "    AP[threshold] = round(calc_AP(threshold, oks), 3)\n",
        "\n",
        "mAP = round(calc_mAP(oks), 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOut2L9zRCMF",
        "outputId": "a218823e-10a4-46db-fbaf-1714e4bd4ac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pck: {0.05: 0.418, 0.2: 0.478, 0.5: 0.532}\n",
            "pdj: {0.05: 0.451, 0.2: 0.498, 0.5: 0.574}\n",
            "AP:  {0.5: 0.65, 0.75: 0.534}\n",
            "mAP: 0.502\n",
            "Average time: 0.745\n"
          ]
        }
      ],
      "source": [
        "print(f\"pck: {pck}\")\n",
        "print(f\"pdj: {pdj}\")\n",
        "print(f\"AP:  {AP}\")\n",
        "print(f\"mAP: {mAP}\")\n",
        "print(f\"Average time: {round(np.mean(time_landmarks), 3)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0hYN-tUQBgz"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Research_Halpe.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c66c9cdcfd9c4dad818afd2a40177730": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_25a1a3f7d87f42ca9997bd8b494d3bce",
              "IPY_MODEL_cdea2a18b73846da9953360235e5c249",
              "IPY_MODEL_1942c3645ada4837ba8ce7a79da9a495"
            ],
            "layout": "IPY_MODEL_6ebbd517f95f4d86ab78c00d641ce543"
          }
        },
        "25a1a3f7d87f42ca9997bd8b494d3bce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52d938e22bdd4b86afd1175ba7dd81d4",
            "placeholder": "​",
            "style": "IPY_MODEL_c9b167db2dee48b0a06fc2bebdb4ec1b",
            "value": "100%"
          }
        },
        "cdea2a18b73846da9953360235e5c249": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0992600298f44c8b91ecf451bf24c06",
            "max": 255011654,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f1fe975e7e8f455c823a3c9af5bf3ea5",
            "value": 255011654
          }
        },
        "1942c3645ada4837ba8ce7a79da9a495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62b2f20a1e2742f19b87f84911f74624",
            "placeholder": "​",
            "style": "IPY_MODEL_fafa7d2f44d444b7a862246940b074a7",
            "value": " 243M/243M [00:29&lt;00:00, 8.68MB/s]"
          }
        },
        "6ebbd517f95f4d86ab78c00d641ce543": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52d938e22bdd4b86afd1175ba7dd81d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9b167db2dee48b0a06fc2bebdb4ec1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0992600298f44c8b91ecf451bf24c06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1fe975e7e8f455c823a3c9af5bf3ea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62b2f20a1e2742f19b87f84911f74624": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fafa7d2f44d444b7a862246940b074a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70b6916e2daf4021a23f7a2be0e3a387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_554a901e2c4d4b7a816cc65fd88296c1",
              "IPY_MODEL_9b2847c8ec65472680993120a817f3c7",
              "IPY_MODEL_d71d138996cd4923acb1c26b109bcd10"
            ],
            "layout": "IPY_MODEL_b19c4df8a201400fa40ef704f804f275"
          }
        },
        "554a901e2c4d4b7a816cc65fd88296c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f92ccf869fc428085042f9299cadd66",
            "placeholder": "​",
            "style": "IPY_MODEL_b7e0de122acc432a82f8ec2e40673aba",
            "value": "100%"
          }
        },
        "9b2847c8ec65472680993120a817f3c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5f98e1dd49940cfa0f444db85caee64",
            "max": 167287506,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_687cefb903784eddbf2582a37770a950",
            "value": 167287506
          }
        },
        "d71d138996cd4923acb1c26b109bcd10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f9648b87d6443018a7ddafc1e0312bf",
            "placeholder": "​",
            "style": "IPY_MODEL_b6618af70aa141b9a6f26d947693b2f1",
            "value": " 160M/160M [00:20&lt;00:00, 7.99MB/s]"
          }
        },
        "b19c4df8a201400fa40ef704f804f275": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f92ccf869fc428085042f9299cadd66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7e0de122acc432a82f8ec2e40673aba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5f98e1dd49940cfa0f444db85caee64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "687cefb903784eddbf2582a37770a950": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f9648b87d6443018a7ddafc1e0312bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6618af70aa141b9a6f26d947693b2f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}